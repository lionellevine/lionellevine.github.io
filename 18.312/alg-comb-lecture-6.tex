\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage{verbatim}

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}


\newcommand{\handout}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox{
      \vbox{
    \hbox to 5.78in {{\sf 18.312: Algebraic Combinatorics} 
\hfill \sf #2 }
       \vspace{4mm}
       \hbox to 5.78in { {\Large \hfill #5  \hfill} }
       \vspace{2mm}
       \hbox to 5.78in { {\em #3 \hfill #4} }
      }
   }
   \end{center}
   \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{Lecture date: #3}{Notes by: #4}{Lecture #1}}


\textwidth=6in
\oddsidemargin=0.25in
\evensidemargin=0.25in
\topmargin=-0.1in
\footskip=0.8in
\parindent=0.0cm
\parskip=0.3cm
\textheight=8.00in
\setcounter{tocdepth} {3}
\setcounter{secnumdepth} {2}
\sloppy

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\newtheorem{answer}[theorem]{Answer}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{example}[theorem]{Example}
\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}

\newcommand{\N}{\mathbb N} % natural numbers 0,1,2,...
\newcommand{\Z}{\mathbb Z}  % integers
\newcommand{\R}{\mathbb R} % reals
\newcommand{\C}{\mathbb C} % complex numbers
\newcommand{\F}{\mathbb F} % finite fields

\newcommand{\floor}[1]{\left\lfloor {#1} \right\rfloor} % floor function
\newcommand{\ceiling}[1]{\left\lceil {#1} \right\rceil} % ceiling function
\newcommand{\binomial}[2]{\left( \begin{array}{c} {#1} \\ 
                        {#2} \end{array} \right)} % binomial coefficients
\newcommand{\modulo}[1]{\quad (\mbox{mod }{#1})} %congruences

\newcommand{\ignore}[1]{} % useful for commenting things out



\begin{document}
\lecture{6}{Lionel Levine}{Feb 17, 2011}{Dennis Tseng} 
% replace n in the line above (and in the file name) by an actual integer
% replace Feb 1 by the date of the lecture 

\section{Reprise of $\frac{d}{dx}=\ln(E)$}
Let $E$ denote the shift operator, such that for a sequence of numbers $s_{0},s_{1},s_{2},\ldots$, 
\begin{equation*}
E(s_{0},s_{1},s_{2},\ldots)=(s_{1},s_{2},s_{3},\ldots). 
\end{equation*}
In the previous lecture, we mentioned the equation
\begin{equation*}
E=e^{\frac{d}{dx}}.
\end{equation*}
%Does the function have to be real-valued?
Also, as mentioned in the last lecture, we can also have $E$ operate on functions. If $f$ is a function, then let 
\begin{equation*}
(Ef)(x)=f(x+1).
\end{equation*}
We can also define $E^{h}$ to be
\begin{equation*}
(E^{h}f)(x)=f(x+h),
\end{equation*}
where $h$ is any real number. 

To better understand the equation $E=d^{\frac{d}{dx}}$, we recall the Taylor expansion of $e^{x}$. 
\begin{equation*}
e^{t}=1+t+\frac{t^2}{2}+\cdots+\frac{t^n}{n!}+\cdots
\end{equation*}
In a similar way, we can think of $e^{t\frac{d}{dx}}$ as
\begin{equation}
\label{eqn1}
e^{t\frac{d}{dx}}=1+t\frac{d}{dx}+\frac{t^2\left(\frac{d}{dx}\right)^2}{2}+\cdots+\frac{t^{n}\left(\frac{d}{dx}\right)^{n}}{n!}+\cdots
\end{equation}
In \eqref{eqn1} above, multiplication of operators is the same as the composition of operators. In particular $\left(\frac{d}{dx}\right)^{n}=\frac{d^{n}}{dx^{n}}$. Now, given the definition \eqref{eqn1}, we can let $e^{t\frac{d}{dt}}$ operate on a function. 
\begin{equation*}
\left[e^{t\frac{d}{dx}}\right](f)=f+tf'+\frac{t^2}{2}f''+\cdots+\frac{t^{n}}{n!}f^{(n)}+\cdots
\end{equation*}
Now, if we plug in $x=0$, we get
\begin{center}
$f(0)+tf'(0)+\frac{t^2}{2}f''(0)+\cdots+\frac{t^{n}}{n!}f^{(n)}(0)+\cdots$,
\end{center}
which is the Taylor series for $f(t)$, so we can write $\left[e^{t\frac{d}{dx}}\right]$ at $x=0$ as $f(t)$. We can also write $f(t)$ by using the shift operator, where $\left[E^{t}f\right](0)=f(t)$. Therefore, $\left[E^{t}f\right](0)=f(t)=\left[e^{t\frac{d}{dx}}f\right](0)$, and $E^{t}=e^{t\frac{d}{dx}}$. When we plug $t=1$, as get $E=e^{\frac{d}{dx}}$, as desired.
\subsection{Eigenvectors and eigenvalues of $E$}
If we look at how $E$ operators on sequences, if the sequence $s_{0},s_{1},s_{2},\ldots$ is an eigenvector of $E$ with eigenvalue $\phi$, then
\begin{align*}
E(s_{0},s_{1},s_{2},\ldots)&=(\phi s_{0},\phi s_{1},\phi s_{2},\ldots)\\
(s_{1},s_{2},s_{3},\ldots)&=(\phi s_{0},\phi s_{1},\phi s_{2},\ldots).
\end{align*}
Therefore, $s_{n+1}=\phi s_{n}$ for all $n\geq 0$, and $s_{n}=s_{0}\phi^{n}$ for all nonnegative integer $n$ and nonzero $s_{0}$. 

Also, using methods learned in a differential equations class, we can show that the eigenvectors of $\frac{d}{dx}$ with eigenvalue $\lambda$ are functions in the form $f(x)=ce^{\lambda x}$ for some constant $c\neq 0$. 

These eigenvectors are essentially the same thing as $s_{n}=s_{0}\phi^{n}=s_{0}e^{\lambda n}$, where $\lambda=\ln(\phi)$. Therefore, if $s$ is the sequence $(s_{0},s_{1},\ldots)$, $Es=\phi s=e^{\lambda}s$ and $\frac{d}{dx}f=\lambda f$. 

The operators $E$ and $\frac{d}{dx}$ have the same eigenvectors $ce^{\lambda x}$ but different eigenvalues. We see that $ce^{\lambda x}$ has eigenvalue $\lambda$ for $\frac{d}{dx}$ and eigenvalue $e^{\lambda}$ for $E$.
\section{Linear Recurrence Sequences}
From previous lectures, we have shown that the following conditions for sequences that satisfy linear recurrences are equivalent. We say that $\{s_{n}\}_{n\geq 0}$ satisfies a linear recurrence of order $k$ if any of the follow is true:
\begin{enumerate}
\item
There exists constants $a_{0},\ldots,a_{k-1}\in\mathbb{C}$ such that
\begin{equation*}
s_{n+k}=\sum_{i=0}^{k-1}{a_{i}s_{n+i}}
\end{equation*}
for all $n\geq 0$. An example of this is $s_{n+3}=2s_{n+2}-5s_{n+1}+s_{n}$. 
\item
The terms of the sequences can be expressed as
\begin{equation*}
s_{n}=\sum_{i=1}^{m}{q_{i}(n)\phi_{i}^{n}},
\end{equation*}
where $\phi_{1},\ldots,\phi_{m}$ are constants in $\mathbb{C}$,$q_{1}(x),q_{2}(x),\ldots,q_{m}(x)$ are polynomials over the complex numbers (in $\mathbb{C}[x]$), and $\displaystyle\sum_{i=1}^{m}{\deg(q_{i})}=k$.
\item
The exponential generating function
\begin{equation*}
\mathcal{F}(x)=\sum_{n\geq 0}{s_{n}\frac{x^{n}}{n!}}
\end{equation*}
satisfies a linear differential equation of order $k$. This is true because you shift the series when you differentiate. 
\end{enumerate}
We will present a couple more equivalent conditions:
\begin{enumerate}
\setcounter{enumi}{3}
\item
The ordinary generating function 
\begin{equation*}
F_{s}(x)=\sum_{n\geq 0}{s_{n}x^{n}}
\end{equation*}
is $\frac{P(x)}{Q(x)}$ for some polynomials $P(x),Q(x)\in\mathbb{C}[x]$ such that $\deg(P)<\deg(Q)\leq k$.
\item
We can express the terms of the sequence as 
\begin{equation*}
s_{n}=v^{t}A^{n}w
\end{equation*}
for some $k$ by $k$ matrix $A=(a_{ij})_{i,j=1}^{k}$ and some vectors $v$ and $w$. 
\end{enumerate}
\subsection{Proof of condition 4}
We will prove the fourth condition is equivalent to the first condition. If $F_{s}(x)=\frac{P(x)}{Q(x)}$, where $Q(x)=\displaystyle\sum_{i=0}^{k}{a_{i}x^{i}}$, we get
\begin{align*}
F_{s}(x)&=\frac{P(x)}{Q(x)}\\
Q(x)F_{s}(x)&=P(x)\\
\left(\sum_{i=0}^{k}{a_{i}x^{i}}\right)\left(\sum_{n\geq 0}{s_{n}x^{n}}\right)&=P(x).
\end{align*}
After expanding we get
\begin{equation*}
\sum_{m\geq 0}{\left(\sum_{i+n=m}{a_{i}s_{n}}\right)x^{m}}=P(x).
\end{equation*}
Now, equate coefficients of $x^{m}$. If $m\geq k$, then the coefficient of $x^{k}$ on the right side is 0, since $P(x)$ must have degree less than $k$. So if $m\geq k$,
\begin{align}
\sum_{i+n=m}{a_{i}s_{n}}&=0\notag\\
\label{eqn2}
\sum_{i=0}^{k}{a_{i}s_{m-i}}&=0.
\end{align}
The equation \eqref{eqn2} is a linear recurrence of order $k$. In the equation $F_{s}(x)=\frac{P(x)}{Q(x)}$, $Q(x)$ encodes the coefficients of the recurrence and $P(x)$ encodes the initial conditions. 

\subsection{Proof of condition 5}
We will prove fifth condition also defines a sequence that satisfies a linear recurrence of order $k$. To do this, we will use the Cayley Hamilton Theorem:
\begin{theorem}
Let $A$ be a square matrix. If $\chi_{A}(x)=\det(xI-A)$ is the characteristic polynomial of $A$, then $\chi_{A}(A)=0$.
\end{theorem}

To show that the fifth definition also defines a sequence that satisfies a linear recurrence of order $k$, we first show that if the $n^{th}$ term of the sequence can be expressed as $v^{t}A^{n}w$ for a $k$ by $k$ matrix $A$ and vectors $v$ and $w$. Let the characteristic polynomial $\chi_{A}(x)$ be $\displaystyle\sum_{i=0}^{k}{c_{i}x^{i}}$. Then, for any nonnegative integer $n$,
\begin{align*}
\sum_{i=0}^{k}{c_{i}s_{n+i}}&=\sum_{i=0}^{k}{c_{i}v^{t}A^{n+i}w}\\
&=v^{t}\left(\sum_{i=0}^{k}{c_{i}A^{n+i}}\right)w\\
&=v^{t}\left[A^{n}\sum_{i=0}^{k}{c_{i}A^{i}}\right]w\\
&=v^{t}\left[A^{n}\chi_{A}(A)\right]w\\
&=v^{t}\left[A^{n}0\right]w\\
&=0.
\end{align*}
One direction is proven. How we need to show that we can represent any linear recurrence in this form. 

Given $S_{n}$ satisfying a linear recurrence 
\begin{equation*}
\sum_{i=0}^{k}{c_{i}s_{n+i}}=0
\end{equation*}
for all nonnegative integer $n$ and $c_{k}=1$, we want to find a matrix $A$ such that its characteristic polynomial is
\begin{equation*}
\chi_{A}(x)=\sum_{i=0}^{k}{c_{i}x^{i}}.
\end{equation*}
One method is to factor $\chi_{A}$ into $\displaystyle\prod_{i=1}^{k}{x-\phi_{i}}$, where $\phi_{i}$ are the roots of $\chi_{A}$ with multiplicity. Then, let 
\begin{equation*}
A=
\begin{bmatrix}
\phi_{1} & 0 & \cdots & 0\\
0 & \phi_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \phi_{k}
\end{bmatrix}.
\end{equation*}
We could also create an integer matrix if $c_{i}$ are integers for all $0\leq i\leq k$. Let $A$ be the matrix
\begin{align*}
A&=
\begin{bmatrix}
0 & 1 &\cdots & 0 & 0\\
0 & 0 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots &\vdots \\
0 & 0 & \cdots & 0 & 1\\
-c_{0} & -c_{1} & \cdots & -c_{k-2} & c_{k-1}
\end{bmatrix},
\end{align*}
where $A$ has 1's on the superdiagonal, -1 times the coefficients of $\chi_{A}$ in the last row, and 0's everywhere else. Then,
\begin{align*}
A
\begin{bmatrix}
s_{n}\\ s_{n+1}\\ \vdots \\ s_{n+k-1}
\end{bmatrix}
&=
\begin{bmatrix}
0 & 1 &\cdots & 0 & 0\\
0 & 0 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots &\vdots \\
0 & 0 & \cdots & 0 & 1\\
-c_{0} & -c_{1} & \cdots & -c_{k-2} & c_{k-1}
\end{bmatrix}
\begin{bmatrix}
s_{n}\\ s_{n+1}\\ \vdots \\ s_{n+k-1}
\end{bmatrix}\\
&=
\begin{bmatrix}
s_{n+1} \\ s_{n+2} \\ \vdots \\ s_{n+k-1} \\
-s_{n}c_{0}-s_{n+1}c_{1}+\cdots-s_{n+k-1}c_{k-1}
\end{bmatrix}.
\end{align*}
Since $\displaystyle\sum_{i=0}^{k}{c_{i}s_{n+i}}=0$, with $c_{k}=1$,
\begin{align*}
\begin{bmatrix}
s_{n+1} \\ s_{n+2} \\ \vdots \\ s_{n+k-1} \\
-s_{n}c_{0}-s_{n+1}c_{1}+\cdots-s_{n+k-1}c_{k-1}
\end{bmatrix}
&=
\begin{bmatrix}s_{n+1}\\ s_{n+2}\\ \vdots \\ s_{n+k}
\end{bmatrix}.
\end{align*}
Now, let 
\begin{center}
$w=\begin{bmatrix} s_{0} \\ s_{1} \\ \vdots \\ s_{k-1}\end{bmatrix}$, $v=\begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0\end{bmatrix}$.
\end{center}
Then, 
\begin{align*}
v^{t}A^{n}w&=v^{t}\begin{bmatrix} s_{n} \\ s_{n+1} \\ \vdots \\ s_{n+k-1}\end{bmatrix}\\
&= s_{n}.
\end{align*}
\subsection{Example of a recurrence problem}
\begin{example}
How many non-self-intersection paths start at the origin in $\mathbb{Z}^2$ with a total of $n$ steps, where all the steps are either up, left, or right?
\end{example}

One example of such a path with 6 steps starts at (0,0) and traverses (1,0), (2,0), (2,1), (3,1), (3,2), (2,2) in order. 

We can encode these paths by words with the letters $N$, $E$, and $W$, where $N$ denotes traveling one unit up, $E$ denotes traveling one unit to the right, and $W$ denotes traveling one unit to the left. The path in the example is given by the word $EENENW$. Since the path must be non-self-intersecting, we are forbidden to have $EW$ or $WE$ as consecutive letters.

Let $f(n)$ be the number of paths with $n$ steps, or the number of words of length $n$ containing the letters $N$, $E$, and $W$ without having $EW$ or $WE$ as consecutive letters. If $n$ is at least 2, there are 7 possibilities for the last 2 letters of such a word: $EN,WN,NN,EE,NE,WW$, or $NW$. The number of words of length $n$ that end in $EN,WN,$ or $NN$ is $f(n-1)$, since there is no restriction on what can come before $N$. Similarly, the number of words that can end in $NW$ is $f(n-2)$.

Now we claim that the number of words that can end in $WW$, $EE$, or $NE$ is $f(n-1)$. Given any valid word with length $n-1$, it ends with either an $E$, $N$ or $W$. If it ends with $E$, append an $E$. If it ends with an $N$, append an $E$. If it ends in a $W$, append an $W$. Therefore, we have found a bijection between the words with length $n-1$ and the words of length $n$ that end with $WW$, $EE$, or $NE$. Therefore, we know $f(n)=2f(n-1)+f(n-2)$. Solving, we get
\begin{align*}
f(n)&=2f(n-1)+f(n-2)\\
f(n)-2f(n-1)-f(n-2)&=0\\
(E^2-2E-1)f&=0\\
(E-(1+\sqrt{2}))(E-(1-\sqrt{2}))f&=0.
\end{align*}
Therefore, $f(n)=a(1+\sqrt{2})^{n}+b(1-\sqrt{2})^{n}$ for some constants $a$ and $b$. We know that $f(0)=1$, since the word of no letters has length 0, and $f(1)=3$. Solving for $a$ and $b$ from these initial conditions yields $a=\frac{1+\sqrt{2}}{2}$ and $b=\frac{1-\sqrt{2}}{2}$. Therefore, we have
\begin{equation*}
f(n)=\frac{1+\sqrt{2}}{2}(1+\sqrt{2})^{n}+\frac{1-\sqrt{2}}{2}(1-\sqrt{2})^{n},
\end{equation*}
for all nonnegative integers $n$. 
\end{document}
